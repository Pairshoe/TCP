###TCP 实验

王拓为 2018011917



####必选部分

#####Step 1. TCP 序列号的对比与生成

* 实现序列号的对比函数（共 4 个）

  * 需要考虑循环溢出情况，因此对于 32 位无符号整数以 `0x80000000` 为界限比较大小
  * 小于、小于等于和大于、大于等于互为对称情况

  ````C++
  bool tcp_seq_lt(uint32_t a, uint32_t b) {
    return 0 < (b - a) && (b - a) < 0x80000000;
  }
  
  bool tcp_seq_le(uint32_t a, uint32_t b) {
    return (b - a) < 0x80000000;
  }
  
  bool tcp_seq_gt(uint32_t a, uint32_t b) {
    return 0 < (a - b) && (a - b) < 0x80000000;
  }
  
  bool tcp_seq_ge(uint32_t a, uint32_t b) {
    return (a - b) < 0x80000000;
  }
  ````

* 实现序列号的生成函数

  * 获取当前微秒时间戳（64 位），取低 32 位后右移 2 位，对 `0xffffffff` 取模，实现每 4 微秒递增

  ````C++
  uint32_t generate_initial_seq() {
    return ((uint32_t)current_ts_usec() >> 2) % 0xffffffff;
  }
  ````



##### Step 2. TCP 三次握手连接的建立

* 对于客户端
  * 首先发送 `SYN`，进入 `SYN_SENT` 状态，框架已经实现
  
  * 在 `SYN_SENT` 状态下，接收到 `SYN+ACK`，发送 `ACK`，进入 `ESTABLISHED` 状态 
    * 更新 `RCV.NXT`、`IRS` 和 `SND.UNA` 的值，更新重传队列
    
    ````C++
    tcp->rcv_nxt = seg_seq + 1;
    tcp->irs = seg_seq;
    if (tcp_header->ack) {
        tcp->snd_una = seg_ack;
        tcp->pop_from_retransmission_queue(seg_ack);
    }
    ````
    
    * 进入 `ESTABLISHED` 状态，发送 `ACK`（以下为发包的实现框架，此后不再重复说明）
    
    ````C++
    // send ACK to remote
    // 40 = 20(IP) + 20(TCP)
    // ip
    uint8_t buffer[40];
    IPHeader *ip_hdr = (IPHeader *)buffer;
    memset(ip_hdr, 0, 20);
    ip_hdr->ip_v = 4;
    ip_hdr->ip_hl = 5;
    ip_hdr->ip_len = htons(sizeof(buffer));
    ip_hdr->ip_ttl = 64;
    ip_hdr->ip_p = 6; // TCP
    ip_hdr->ip_src = ip->ip_dst;
    ip_hdr->ip_dst = ip->ip_src;
    // tcp
    TCPHeader *tcp_hdr = (TCPHeader *)&buffer[20];
    memset(tcp_hdr, 0, 20);
    tcp_hdr->source = tcp_header->dest;
    tcp_hdr->dest = tcp_header->source;
    tcp_hdr->seq = htonl(tcp->snd_nxt);
    tcp_hdr->ack_seq = htonl(tcp->rcv_nxt);
    // flags
    tcp_hdr->doff = 20 / 4; // 20 bytes
    tcp_hdr->ack = 1;
    // window size
    tcp_hdr->window = htons(tcp->recv.free_bytes());
    // update checksum
    update_tcp_ip_checksum(buffer);
    // send packet
    send_packet(buffer, sizeof(buffer));
    ````
    
    * 更新 `SND_WND`、`SND_WL1` 和 `SND_WL2` 的值
    
    ````C++
    tcp->snd_wnd = seg_wnd;
    tcp->snd_wl1 = seg_seq;
    tcp->snd_wl2 = seg_ack;
    ````
  
* 对于服务端
  * 在 `LISTEN` 状态下，接收到 `SYN`，新建 TCP socket 后发送 `SYN+ACK`，进入 `SYN_RCVD` 状态
    * 需要添加发送 `SYN+ACK` 的实现，在发包实现框架的基础上将 `SYN` 和 `ACK` 置 1
  * 在 `SYN_RCVD` 状态下，接收到 `ACK`，进入 `ESTABLISHED` 状态，框架已经实现



#####Step 3. 简易的发送和接收逻辑

* 实现接收数据逻辑

  * 首先发送 `SYN` 时，设置窗口大小为接收缓冲区剩余字节数

    ````C++
    tcp_hdr->window = htons(tcp->recv.free_bytes());
    ````

  * 在 `ESTABLISHED` 和 `FIN_WAIT_1` 状态下，接收到 `ACK`

    * 更新 `SND.UNA`

    ````C++
    if (tcp_seq_lt(tcp->snd_una, seg_ack) && tcp_seq_le(seg_ack, tcp->snd_nxt)) {
        tcp->snd_una = seg_ack;
    }
    ````

    * 更新 `SND_WND`、`SND_WL1` 和 `SND_WL2`

    ````C++
    if (tcp_seq_lt(tcp->snd_wl1, seg_seq) || 
        ((tcp->snd_wl1 == seg_seq) && tcp_seq_le(tcp->snd_wl2, seg_ack))) {
        tcp->snd_wnd = seg_wnd << tcp->wnd_shift_cnt;
        tcp->snd_wl1 = seg_seq;
        tcp->snd_wl2 = seg_ack;
    }
    ````

    * 如果包中含有数据

      * 将数据写入接收缓冲区

      ````C++
      size_t res = tcp->recv.write(payload, seg_len);
      ````

      * 更新 `RCV.NXT` 和 `RCV.WND` 状态

      ````C++
      tcp->rcv_nxt = tcp->rcv_nxt + res;
      tcp->rcv_wnd = tcp->recv.free_bytes();
      ````

      * 发送 `ACK`，在发包实现框架的基础上将 `ACK` 置 1

* 实现 TCP 发送接口

  * 在 `SYN_SENT` 和 `SYN_RCVD` 状态下，写入发送缓冲区，框架已经实现

  * 在 `ESTABLISHED` 和 `CLOSE_WAIT` 状态下

    * 写入发送缓冲区

    ````C++
    size_t res = tcp->send.write(data, size);
    ````

    * 综合 `MSS`、待发送的数据大小和对方窗口大小，发送数据

    ````C++
    // send data to remote
    size_t bytes_to_send = tcp->send.size;
    // initialize bytes_have_sent
    uint32_t bytes_have_sent = tcp->snd_nxt - tcp->snd_wl2;
    // constraint by remote rcv_wnd, which is equal to snd_wnd
    while (bytes_to_send && bytes_have_sent <= tcp->snd_wnd) {
        // constraint by remote_mss
        size_t segment_len = min(bytes_to_send, tcp->remote_mss);
        // update bytes_to_send
        bytes_to_send -= segment_len;
        // split into segments
        if (segment_len > 0) {
            printf("Sending segment of len %zu to remote\n", segment_len);
            // send data now
            // 20 IP header & 20 TCP header
            uint16_t total_length = 20 + 20 + segment_len;
            uint8_t buffer[MTU];
            ... ...
            // set ack bit and ack_seq
            tcp_hdr->ack = 1;
            tcp_hdr->ack_seq = htonl(tcp->rcv_nxt);
            // window size: size of empty bytes in recv buffer
            tcp_hdr->window = htons(tcp->recv.free_bytes());
            // payload
            size_t bytes_read = tcp->send.read(&buffer[52], segment_len);
            // should never fail
            assert(bytes_read == segment_len);
            // update checksum
            update_tcp_ip_checksum(buffer);
            // send packet
            send_packet(buffer, total_length);
            // update bytes_have_sent
            bytes_have_sent += segment_len;
        }
    }
    return res;
    ````

* 实现读取接收缓冲区接口

  * 将接收缓冲区数据拷贝至用户数据

  ````C++
  ssize_t tcp_read(int fd, uint8_t *data, size_t size) {
      TCP *tcp = tcp_fds[fd];
      assert(tcp);
      return tcp->recv.read(data, size);
  }
  ````



##### Step 4. TCP 连接的终止

* 实现关闭连接接口，发送 `FIN`

  * 由于在发送 `FIN` 前应保证所有数据已经完成传输，即重传队列为空，故需要设计阻塞逻辑

    * 实现 `FIN` 定时器对应回调函数 `Fin`

      回调函数实现的功能是以 200ms 为间隔检查重传队列是否为空，如果为空则发送 `FIN`

    ````C++
    // fin timer
    struct Fin {
      int fd;
      size_t operator()() {
        printf("|* Fin *|\n");
        TCP *tcp = tcp_fds[fd];
        assert(tcp);
        if (tcp->retransmission_queue.empty()) {
          tcp_shutdown(fd);
          return -1;
        } else {
          return 200;
        }
      }
    };
    ````

  * 在函数 `tcp_shutdown` 中实现关闭连接逻辑，检查传输队列是否为空

    * 若为空，则发送 `FIN`，根据此刻状态不同进入对应后续状态
    * 若不为空，则在实验框架的 `TIMERS` 结构体中注册回调函数

    ````C++
    Fin fin_fn;
    fin_fn.fd = fd;
    TIMERS.add_job(fin_fn, current_ts_msec());
    ````

* 实现接收 `FIN` 逻辑

  * 在 `CLOSED`、`LISTEN` 和 `SYN_SENT` 状态下，不做处理，直接返回
  * 其余状态下，更新 `RCV.NXT`（注意 `FIN` 占 1 个包序号），发送 `ACK` 

  ````C++
  // advance RCV.NXT over the FIN
  tcp->rcv_nxt = seg_seq + seg_len + 1;
  ````

  * 在 `SYN_RCVD` 和 `ESTABLISHED` 状态下，进入 `CLOSE_WAIT` 状态，框架已经实现

  * 在 `FIN_WAIT_1` 状态下

    * 如果此前发送的 `FIN` 被 `ACK`，开启 `TIME_WAIT` 定时器，进入 `TIME_WAIT` 状态

      * 实现 `TIME_WAIT` 定时器对应回调函数 `Time_wait`

        回调函数实现了功能时以 2000ms 为间隔检查是否超时，其中超时的判定条件为：

        判定时间和定时器开启时间（存储在 `time_wait_timer` 中）间隔大于 2 倍 `MSL`

        参考 Linux 规范，`MSL` 被设置为了 30s

      ````C++
      // time_wait timer
      struct Time_wait {
          int fd;
          size_t operator()() {
              TCP *tcp = tcp_fds[fd];
              assert(tcp);
              if (tcp->state == TCPState::TIME_WAIT) {
                  if (2 * MSL < current_ts_msec() - tcp->time_wait_timer) {
                      tcp->set_state(TCPState::CLOSED);
                      return -1;
                  } else {
                      return 2000;
                  }
              }
          }
      };
      ````

      * 在实验框架的 `TIMERS` 结构体中注册回调函数

      ````C++
      Time_wait time_wait_fn;
      time_wait_fn.fd = pair.first;
      uint64_t current_ms = current_ts_msec();
      tcp->time_wait_timer = current_ms;
      TIMERS.add_job(time_wait_fn, current_ms);
      ````

      * 关闭其他定时器，此时位于传输层的定时器可能有 3 种
        * 检查重传队列是否超时的定时器，而重传队列在发送 `FIN` 前应已清空
        * 检查 Nagle 算法缓冲区是否超时的定时器，而 Nagle 算法缓冲区发送 `FIN` 前应已清空
        * 检查发送 `FIN` 前重传队列是否已清空的定时器，此时应已完成退出

    * 否则，进入 `CLOSING` 状态

  * 在 `FIN_WAIT_2` 状态下，开启 `TIME_WAIT` 计时器，进入 `TIME_WAIT` 状态，实现与上述相同



##### Step 5. 访问百度主页

* 在前 4 步实现的基础上，可以直接通过

* 截止该阶段，全部的自动化测试通过截图如下：

  ![截屏2022-05-13 14.56.36](https://img.wzf2000.top/image/2022/05/13/2022-05-13-14.56.363bb709df7aa9c2d2.png)



##### Step 6. 重传和乱序重排

* 重传的实现思路是，对于发出去的 TCP 分组，记录在重传队列中，并且启动一个定时器，每一段时间检查一下列表中的 TCP 分组，如果发现有已经被对端 ACK 的，就删掉；否则就再次发送

  * 实现记录 TCP 分组的数据结构 `Segment`

  ````C++
  struct Segment {
    size_t header_len; // segment header length
    size_t body_len; // segment body length
    uint8_t buffer[MTU]; // segment data (< MTU bytes)
    uint64_t start_ms; // the time when segment push to the retransmission queue
    size_t dup_ack_cnt; // the duplicate ACK count for later Reno's implementation
  
    Segment() { header_len = body_len = dup_ack_cnt = 0; start_ms = 0; }
    Segment(const uint8_t *_buffer, const size_t _header_len, const size_t _body_len, const uint64_t _start_ms) {
      header_len = _header_len;
      body_len = _body_len;
      memcpy(buffer, _buffer, _header_len + _body_len);
      start_ms = _start_ms;
      dup_ack_cnt = 0;
    }
  };
  ````

  * 实现重传队列 `retransmission_queue` 为以 `Segment` 为元素的向量

  ````C++
  std::vector<Segment> retransmission_queue;
  ````

  * 当发出需要重传的 TCP 分组时，调用函数 `push_to_retransmission_queue` 将其加入重传队列

  ````C++
  void TCP::push_to_retransmission_queue(const uint8_t *buffer, const size_t header_len, const size_t body_len) {
      TCPHeader *tcp_hdr = (TCPHeader *)&buffer[20];
      uint32_t seq = ntohl(tcp_hdr->seq);
      // check retransmission queue
      for (auto seg : retransmission_queue) {
          TCPHeader *seg_tcp_hdr = (TCPHeader *)&seg.buffer[20];
          uint32_t seg_seq = ntohl(seg_tcp_hdr->seq);
          if (seg_seq == seq) {
              // already in retransmission queue
              return;
          }
      }
      // push packet
      Segment new_seg = Segment(buffer, header_len, body_len, current_ts_msec());
      retransmission_queue.push_back(new_seg);
  }
  ````

  * 当收到 `ACK` 时，调用函数 `pop_from_retransmission_queue` 检查重传队列中是否有可以删掉的分组

  ````C++
  void TCP::pop_from_retransmission_queue(const uint32_t seg_ack) {
      ssize_t index = -1;
      for (ssize_t i = 0, iEnd = retransmission_queue.size(); i < iEnd; i++) {
          auto& seg = retransmission_queue[i];
          TCPHeader *tcp_hdr = (TCPHeader *)&seg.buffer[20];
          uint32_t seg_seq = ntohl(tcp_hdr->seq);
          // match segment category
          if (0 < seg.body_len) { // segment with payload
              if (seg_seq == seg_ack - seg.body_len) {
                  index = i;
                  break;
              }
          } else { // segment without payload, e.g. SYN, FIN
              if (seg_seq == seg_ack - 1) {
                  index = i;
                  break;
              }
          }
      }
      // new ACK
      if (index != -1) {
          // pop packet
          retransmission_queue.erase(retransmission_queue.begin(), retransmission_queue.begin() + index + 1);
          // "the SYN/ACK and the acknowledgment of the
          // SYN/ACK MUST NOT increase the size of the congestion window."
          if (state != TCPState::SYN_SENT && state != TCPState::SYN_RCVD) {
              clear_dup_ack_cnt();
          }
      }
  }
  ````

  * 实现定时器 `Retransmission` 以 2000ms 为间隔检查重传队列

  ````C++
  // retransmission timer
  struct Retransmission {
    int fd;
    size_t operator()() {
      TCP *tcp = tcp_fds[fd];
      assert(tcp);
      if (tcp->retransmission_queue.empty()) {
        return -1;
      } else {
        tcp->retransmission();
        return 2000;
      }
    }
  };
  ````

  * 实现函数 `retransmission()` 检查重传队列是否有分组需要超时重发，以 `RTO` 作为判断依据

  ````C++
  void TCP::retransmission() {
      uint64_t current_ms = current_ts_msec();
      // check retransmission queue
      for (auto seg : retransmission_queue) {
          // check if time interval is greater than RTO
          if (RTO < current_ms - seg.start_ms) {
              // retransmit packet
              send_packet(seg.buffer, seg.header_len + seg.body_len);
          }
      }
  }
  ````

* 乱序重排的实现思路是，如果发现 TCP 分组的序列号不等于 `RCV.NXT`，此时出现了乱序，先把数据保存到乱序队列中，当之后接受到序列号等于 `RCV.NXT` 的分组时，再将队列中已有的数据拼接起来，写入到接收缓冲区中。

  * 实现记录乱序数据的数据结构 `Payload`

  ````C++
  struct Payload {
    uint32_t seg_seq; // sequence number of payload
    size_t len; // length of payload
    uint8_t data[MTU]; // payload data
  
    Payload() { seg_seq = 0; len =  0; }
    Payload(const uint8_t *_data, const size_t _len, const uint32_t _seg_seq) {
      seg_seq = _seg_seq;
      len = _len;
      memcpy(data, _data, _len);
    }
  };
  ````

  * 实现乱序队列 `out_of_order_queue` 为以 `Payload` 为元素的向量

  ````C++
  std::vector<Payload> out_of_order_queue;
  ````

  * 当收到序列号不等于 `RCV.NXT` 的分组时，调用函数 `push_to_out_of_order_queue` 将其加入乱序队列

  ````C++
  void TCP::push_to_out_of_order_queue(const uint8_t *data, const size_t len, const uint32_t seg_seq) {
    Payload payload = Payload(data, len, seg_seq);
    out_of_order_queue.push_back(payload);
  }
  ````

  * 当接收到新的数据时，调用函数 `reorder` 检查乱序队列查看是否数据可以重组

    函数 `reorder` 接收新数据对应后继的序列号为参数，递归检查是否可以重组

  ````C++
  void TCP::reorder(const uint32_t seg_seq) {
      ssize_t index = -1;
      for (ssize_t i = 0, iEnd = out_of_order_queue.size(); i < iEnd; i++) {
          Payload payload = out_of_order_queue[i];
          // if find successor
          if (seg_seq == payload.seg_seq) {
              // write to the recv buffer
              size_t res = recv.write(payload.data, payload.len);
              rcv_nxt = rcv_nxt + res;
              rcv_wnd = recv.free_bytes();
              index = i;
              break;
          }
      }
      if (index != -1) {
          // update seg_seq
          const uint32_t new_seg_seq = out_of_order_queue[index].seg_seq + out_of_order_queue[index].len;
          // pop written payload from queue
          out_of_order_queue.erase(out_of_order_queue.begin() + index);
          // iterate
          if (!out_of_order_queue.empty()) {
              reorder(new_seg_seq);
          }
      }
  }
  ````

* 实验结果

  * 重传

  使用 `lab-client` 与 `lab-server` 通信，通过命令行参数指定 `lab-server` 的 `-R 0.5`，即接收时的丢包率为 0.5

  从 `lab-server.pcap` 可以看出，`lab-server` 接收发生了多次数据包丢失，通过 `lab-server.log` 看到累计有 13 次之多，但是在 `lab-client` 重传机制的保证下协议栈依然正常工作：

  <img src="https://img.wzf2000.top/image/2022/05/14/2022-05-14-11.00.14.png" alt="截屏2022-05-14 11.00.14" style="zoom:50%;" />

  从 `lab-client.pcap` 可以看出，`lab-client` 在发生丢包时完成了重传，保证了协议栈的正常工作：

  <img src="https://img.wzf2000.top/image/2022/05/14/2022-05-14-10.58.15.png" alt="截屏2022-05-14 10.58.15" style="zoom:50%;" />

  * 乱序重排

  使用 `lab-client` 与 `lab-server` 通信，修改 `common.cpp` 打乱 `lab-server` 发送数据顺序

  ````C++
  void send_packet(const uint8_t *data, size_t size) {
      if (send_delay_max != 0.0) {
          // simulate transfer latency
          double time = send_delay_min + ((double)rand() / RAND_MAX) *
              (send_delay_max - send_delay_min);
          printf("Delay in %.2lf ms\n", time);
  
          delay_sender fn;
          fn.data.insert(fn.data.begin(), data, data + size);
          TIMERS.schedule_job(fn, time);
      } else {
          if (0 < out_of_order_size && 44 < size) {
              packets.push_back(Packet(data, size));
              out_of_order_size--;
              if (out_of_order_size == 0) {
                  // Reverse Order
                  for (auto it = packets.rbegin(); it != packets.rend(); it++) {
                      send_packet_internal(it->data, it->size);
                  }
                  packets.clear();
              }
          } else {
              send_packet_internal(data, size);
          }
      }
  }
  ````

  通过指定 `out_of_order_size = 3` 实现前 3 个数据包的倒序发送，具体地：

  正常情况下，`lab-server` 发送数据顺序为：

  ````tex
  "HTTP/1.1 200 OK\r\n",
  "Content-Length: 13\r\n",
  "Content-Type: text/plain; charset=utf-8\r\n",
  "\r\n",
  "Hello World!\n",
  ````

  修改后，`lab-server` 发送数据顺序为：

  ````tex
  "Content-Type: text/plain; charset=utf-8\r\n",
  "Content-Length: 13\r\n",
  "HTTP/1.1 200 OK\r\n",
  "\r\n",
  "Hello World!\n",
  ````

  从 `lab-server.pcap` 可以看出，`lab-server` 发送的数据为乱序：

  <img src="https://img.wzf2000.top/image/2022/05/14/2022-05-14-10.40.44.png" alt="截屏2022-05-14 10.40.44" style="zoom:50%;" />

  从 `lab-client.pcap` 可以看出，`lab-client` 接收的数据为乱序，且完成了正常的处理：

  <img src="https://img.wzf2000.top/image/2022/05/14/2022-05-14-10.42.58.png" alt="截屏2022-05-14 10.42.58" style="zoom:50%;" />

  打开 `lab-client.log` 可以查看更详细的信息：

  ```bash
  RX: 45 00 00 51 00 00 00 00 40 06 66 A5 0A 00 00 01 0A 00 00 02 00 50 B7 06 EB EE 2A 2D EB EE 2A 28 50 10 28 00 0E 3A 00 00 43 6F 6E 74 65 6E 74 2D 54 79 70 65 3A 20 74 65 78 74 2F 70 6C 61 69 6E 3B 20 63 68 61 72 73 65 74 3D 75 74 66 2D 38 0D 0A
  Received 41 bytes from server
  |* Push Out of Order Queue *|
  out_of_order queue size = 1
  TX: 45 00 00 28 00 00 00 00 40 06 66 CE 0A 00 00 02 0A 00 00 01 B7 06 00 50 EB EE 2A 28 EB EE 2A 08 50 10 28 00 90 6D 00 00
  RX: 45 00 00 3C 00 00 00 00 40 06 66 BA 0A 00 00 01 0A 00 00 02 00 50 B7 06 EB EE 2A 19 EB EE 2A 28 50 10 28 00 5D 36 00 00 43 6F 6E 74 65 6E 74 2D 4C 65 6E 67 74 68 3A 20 31 33 0D 0A
  Received 20 bytes from server
  |* Push Out of Order Queue *|
  out_of_order queue size = 2
  TX: 45 00 00 28 00 00 00 00 40 06 66 CE 0A 00 00 02 0A 00 00 01 B7 06 00 50 EB EE 2A 28 EB EE 2A 08 50 10 28 00 90 6D 00 00
  RX: 45 00 00 39 00 00 00 00 40 06 66 BD 0A 00 00 01 0A 00 00 02 00 50 B7 06 EB EE 2A 08 EB EE 2A 28 50 10 28 00 D0 96 00 00 48 54 54 50 2F 31 2E 31 20 32 30 30 20 4F 4B 0D 0A
  Received 17 bytes from server
  |* Reorder *|
  out_of_order queue size = 1
  |* Reorder *|
  out_of_order queue size = 0
  TX: 45 00 00 28 00 00 00 00 40 06 66 CE 0A 00 00 02 0A 00 00 01 B7 06 00 50 EB EE 2A 28 EB EE 2A 56 50 10 27 B2 90 6D 00 00
  Read 'HTTP/1.1 200 OK
  Content-Length: 13
  Content-Type: text/plain; charset=utf-8
  ' from tcp
  ```

  可以看到，`lab-client` 首先将最先接收到的 2 个乱序数据包加入了乱序队列，当收到第 3 个数据包时，调用重排函数 `reorder`，递归 2 次完成对 3 个数据包的重排，写入接收缓冲区，顺序读出：

  ````tex
  HTTP/1.1 200 OK\r\n
  Content-Length: 13\r\n
  Content-Type: text/plain; charset=utf-8\r\n
  ````



##### Step 7. 实现 Nagle 算法

* Nagle 算法要解决的是小包问题：如果用户 `write` 调用的数据都很小，并且每次 `write` 都要发送一个 TCP 包给对端，就会产生大量的 TCP 包

* 实现思路是，如果 `write` 的数据太小，那就等一小段时间，期望用户在这个时间里继续写入一些数据，直到超时或者积攒到足够大的数据可以发送

  * 实现 Nagle 缓冲区数据结构，`nagle_buffer_size` 记录缓冲区数据大小，`nagle_buffer` 存储缓冲区数据，缓冲区大小设置为 `MTU`

  ````C++
   size_t nagle_buffer_size;
   uint8_t nagle_buffer[MTU];
  ````

  * 修改 TCP 发送数据接口 `tcp_write` 实现，当用户写入数据太小时，不将数据写入发送缓冲区而是写入 Nagle 数据缓冲区，等待足够大或超时再发送

  ````C++
  ssize_t tcp_write(int fd, const uint8_t *data, size_t size) {
      ... ...
      // nagle's algorithm
      if (bytes_to_send < NAGLE_SIZE) {
          // write to nagle buffer
          tcp->nagle_timer = current_ts_msec();
          // payload
          size_t bytes_read = tcp->send.read(&tcp->nagle_buffer[tcp->nagle_buffer_size], bytes_to_send);
          // should never fail
          assert(bytes_read == bytes_to_send);
          tcp->nagle_buffer_size += bytes_read;
          // register nagle timer
          Nagle nagle_fn;
          nagle_fn.fd = fd;
          TIMERS.add_job(nagle_fn, current_ts_msec());
          // check if nagle buffer is big enough
          if (NAGLE_SIZE < tcp->nagle_buffer_size) {
              tcp->clear_nagle_buffer();
          } else {
              printf("|* Nagle Saved %zu *|\n", tcp->nagle_buffer_size);
          }
      } else {
          ... ...
      }
      ... ...
  }
  ````

  * 数据足够大的判断标准 `NAGLE_SIZE` 设置为 40 bytes
  * 超时检测使用定时器实现，以 1000ms 为间隔检查 Nagle 数据缓冲区是否存在超时数据需要发送，超时的判断标准设置为 `2 * RTO`

  ````C++
  // nagle timer 
  struct Nagle {
      int fd;
      size_t operator()() {
          TCP *tcp = tcp_fds[fd];
          assert(tcp);
          if (tcp->nagle_buffer_size != 0) {
              if (2 * RTO < current_ts_msec() - tcp->nagle_timer) {
                  tcp->clear_nagle_buffer();
                  return -1;
              } else {
                  return 1000;
              }
          }
      }
  };
  ````

  * 函数 `clear_nagle_buffer` 负责 Nagle 数据缓冲区的清空和数据的发送

  ````C++
  void TCP::clear_nagle_buffer() {
      size_t segment_len = min(nagle_buffer_size, remote_mss);
      if (segment_len > 0) {
          printf("Sending segment of len %zu to remote\n", segment_len);
          // send data now
  		... ...
          // payload
          memcpy(&buffer[52], nagle_buffer, segment_len);
          // clear nagle buffer
          memset(nagle_buffer, 0, segment_len);
          nagle_buffer_size = 0;
          
          update_tcp_ip_checksum(buffer);
          send_packet(buffer, total_length);
          
          // update full ack
          update_full_ack();
          // push to retransmission queue
          push_to_retransmission_queue(buffer, 40, segment_len);
          // start retransmission timer
          Retransmission retransmission_fn;
          retransmission_fn.fd = fd;
          TIMERS.add_job(retransmission_fn, current_ts_msec());
      }
  }
  ````

* 实验结果

  使用 `lab-client` 与 `lab-server` 通信，设置 `NAGLE_SIZE` 为 40 bytes

  从 `lab-client.pcap` 可以看出，原本分为多个小数据包发送的数据合并成了大的数据包合并发送，同时协议栈正常工作：

  <img src="https://img.wzf2000.top/image/2022/05/14/2022-05-14-23.00.39.png" alt="截屏2022-05-14 23.00.39" style="zoom:50%;" />



##### Step 8. 实现慢启动，冲突避免和快速重传

* 这部分内容我在实验要求的基础上，实现了完整的 Reno 拥塞控制算法，如下图所示：

  <img src="https://img.wzf2000.top/image/2022/05/14/IMG_7306.png" alt="IMG_7306" style="zoom:40%;" />

  * 实现状态机的表示，增加枚举状态变量 `RenoState`

  ````C++
  enum RenoState {
    SLOW_START,
    CONGESTION_AVOIDANCE,
    FAST_RECOVERY,
  };
  ````

  * 实现重复 `ACK` 的计数

    * 在重传队列元素 `Segment` 中增加 `dup_ack_cnt` 字段
    * 修改函数 `pop_from_retransmission_queue`，统计重复 `ACK`

    ````C++
    void TCP::pop_from_retransmission_queue(const uint32_t seg_ack) {
        ssize_t index = -1;
        for (ssize_t i = 0, iEnd = retransmission_queue.size(); i < iEnd; i++) {
            auto& seg = retransmission_queue[i];
            TCPHeader *tcp_hdr = (TCPHeader *)&seg.buffer[20];
            uint32_t seg_seq = ntohl(tcp_hdr->seq);
            // duplicate ACK
            if (seg_seq == seg_ack) {
                if (reno_state == RenoState::SLOW_START ||
                    reno_state == RenoState::CONGESTION_AVOIDANCE) {
                    // dupACKcount++
                    seg.dup_ack_cnt++;
                    // when duplicate ACKs count == 3
                    if (seg.dup_ack_cnt == 4) {
    					... ...
                    }
                } else {
    				... ...
                }
            }
            // match packet
    		... ...
        }
    	... ...
    }
    ````

  * 实现状态机的转换和 `cwnd`、`ssthresh` 的更新

    * 当接收新的 `ACK` 时，进行对应状态转换

    在函数 `pop_from_retransmission_queue` 中实现相关逻辑

    ````C++
    void TCP::pop_from_retransmission_queue(const uint32_t seg_ack) {
        ssize_t index = -1;
        for (ssize_t i = 0, iEnd = retransmission_queue.size(); i < iEnd; i++) {
            ... ...
        }
        // new ACK
        if (index != -1) {
            retransmission_queue.erase(retransmission_queue.begin(), retransmission_queue.begin() + index + 1);
            // "the SYN/ACK and the acknowledgment of the
            // SYN/ACK MUST NOT increase the size of the congestion window."
            if (state != TCPState::SYN_SENT && state != TCPState::SYN_RCVD) {
                // update cwnd
                if (reno_state == RenoState::SLOW_START) {
                    cwnd += DEFAULT_MSS;
                    if (cwnd == ssthresh) {
                        set_reno_state(RenoState::CONGESTION_AVOIDANCE);
                    }
                } else if (reno_state == RenoState::CONGESTION_AVOIDANCE) {
                    delta_cwnd += DEFAULT_MSS;
                    if (delta_cwnd == cwnd) {
                        cwnd += DEFAULT_MSS;
                        delta_cwnd = 0;
                    }
                } else {
                    cwnd = ssthresh;
                    set_reno_state(RenoState::CONGESTION_AVOIDANCE);
                }
                // dupACKcount = 0
                clear_dup_ack_cnt();
            }
        }
    }
    ````

    * 当重复 `ACK` 数量达到 3 时，进行对应状态转换

    在函数 `pop_from_retransmission_queue` 中实现相关逻辑，实现快速重传

    ````C++
    void TCP::pop_from_retransmission_queue(const uint32_t seg_ack) {
        ssize_t index = -1;
        for (ssize_t i = 0, iEnd = retransmission_queue.size(); i < iEnd; i++) {
            auto& seg = retransmission_queue[i];
            TCPHeader *tcp_hdr = (TCPHeader *)&seg.buffer[20];
            uint32_t seg_seq = ntohl(tcp_hdr->seq);
            // duplicate ACK
            if (seg_seq == seg_ack) {
                if (reno_state == RenoState::SLOW_START ||
                    reno_state == RenoState::CONGESTION_AVOIDANCE) {
                    // dupACKcount++
                    seg.dup_ack_cnt++;
                    // when duplicate ACKs count == 3
                    if (seg.dup_ack_cnt == 4) {
                        set_reno_state(RenoState::FAST_RECOVERY);
                        // ssthresh = cwnd / 2; cwnd = ssthresh + 3 * MSS
                        ssthresh = cwnd >> 1;
                        cwnd = ssthresh + 3 * DEFAULT_MSS;
                        // retransmit missing segment
                        send_packet(seg.buffer, seg.header_len + seg.body_len);
                    }
                } else {
                    // cwnd = cwnd + MSS
                    cwnd += DEFAULT_MSS;
                }
            }
            // match packet
    		... ...
        }
        // new ACK
        ... ...
    }
    ````

    * 当出现超时时，进行对应状态转换

    在函数 `retransmission` 中实现相关逻辑

    ```C++
    void TCP::retransmission() {
        uint64_t current_ms = current_ts_msec();
        for (auto seg : retransmission_queue) {
            if (RTO < current_ms - seg.start_ms) {
                set_reno_state(RenoState::SLOW_START);
                // cwnd = MSS; ssthresh = cwnd / 2;
                ssthresh = cwnd >> 1;
                cwnd = DEFAULT_MSS;
                // dupACKcount = 0
                clear_dup_ack_cnt();
                // retransmit missing segment
                send_packet(seg.buffer, seg.header_len + seg.body_len);
            }
        }
    }
    ```

    * 根据阶段的不同，`cwnd` 有两种不同的增长方式
      * 在慢启动阶段，`cwnd` 为每收到一个新的 `ACK`，增加一个 `MSS` 大小，在一个 `RTT` 内为指数增长
      * 在冲突避免阶段，`cwnd` 为在一个 `RTT` 内增加一个 `MSS` 大小为线性增长
      * 由于 `RTT` 在现有框架中难以界定，因此引入 `delta_cwnd` 变量，每接收新的 `ACK` 时增加 `MSS`，当 `delta_cwnd == cwnd` 时，便视为一个 `RTT` 结束，将 `cwnd` 增加一个 `RTT`，将 `delta_cwnd` 置为 0

* 实验结果

  * 慢启动与冲突避免

  使用 `lab-client` 与 `lab-server` 通信

  由于 `cwnd` 和 `ssthresh` 的变化过程需要一个较长的过程，而现有协议栈客户端和服务端的发送数据是固定的，因此我修改了实验框架中 `DEFAULT_MSS` 的大小为 5 bytes，从而在发送数据总量不变的情况下减小了每次发送数据包的大小，也就增加了发包的次数，延长了运行时间

  从 `lab-client.pcap` 可以看出，`lab-client` 发送的窗口大小被 `cwnd` 很好地限制，同时随着发送的数据包被不断确认，`cwnd` 的大小也在以先指数后线性的趋势变化：

  <img src="https://img.wzf2000.top/image/2022/05/15/2022-05-15-00.24.00.png" alt="截屏2022-05-15 00.24.00" style="zoom:50%;" />

  打开 `lab-client.log` 可以更清晰地看到 `cwnd` 和 `ssthresh` 的变化趋势：

  ````bash
  === 1st RTT ===
  # init
  slow start: cwnd = 10 ssthresh = 40
  === 2nd RTT ===
  # 1 ACK
  slow start: cwnd = 15 ssthresh = 40
  # 1 ACK
  slow start: cwnd = 20 ssthresh = 40
  === 3rd RTT ===
  # 1 ACK
  slow start: cwnd = 25 ssthresh = 40
  # 1 ACK
  slow start: cwnd = 30 ssthresh = 40
  # 1 ACK
  slow start: cwnd = 35 ssthresh = 40
  # 1 ACK
  slow start: cwnd = 40 ssthresh = 40
  === 4th RTT ===
  # 1 ACK
  # 1 ACK
  # 1 ACK
  # 1 ACK
  # 1 ACK
  # 1 ACK
  # 1 ACK
  # 1 ACK
  congestion avoidance: cwnd = 45 ssthresh = 40
  ````

  * 快速重传

  使用 `lab-client` 与 `lab-server` 通信，修改 `common.cpp` 指定 `lab-server` 在接收第 11 个数据包的时候发生丢失

  从 `lab-client.pcap` 可以看出，`lab-server` 在丢失 `Seq=41` 的包后发送了 4 个重复的 `ACK`，`lab-client` 在接收到 4 个重复的 `ACK` 后启动快速重传机制，重传了 `Seq=41` 的包

  <img src="https://img.wzf2000.top/image/2022/05/15/2022-05-15-01.01.57.png" alt="截屏2022-05-15 01.01.57" style="zoom:50%;" />



####选做部分

##### Step 1. 实现 TCP Window Scale Option

* Window Scale 选项的长度为 3，但为了保持 TCP 的包长度为 4 的倍数，需要添加 1 个 `NOP` 的空位

  第 1 个字节为选项种类，第 2 个字节为选项长度，第 3 个字节为扩大系数 `shift_cnt`，本次设置为 6，即将窗口扩大 `2^6` 倍

  在发送 `SYN` 的位置添加选项设置，注意 `buffer` 和 `doff` 也要相应增大

````C++
// window scale
buffer[57] = 0x03;
buffer[58] = 0x03;
buffer[59] = 0x06; // shift cnt = 6 -> scale factor = 64
````

* 改变 `snd_wnd` 的计算方式，其中 `wnd_shift_cnt` 为对方此前传输的 `shift_cnt` 大小

````C++
tcp->snd_wnd = seg_wnd << tcp->wnd_shift_cnt;
````

* 实验结果

从 `lab-client.pcap` 可以看出，Window Scale 已经被 WireShark 顺利解析，并且应用在了窗口大小的计算上

<img src="https://img.wzf2000.top/image/2022/05/15/2022-05-15-16.25.01.png" alt="截屏2022-05-15 16.25.01" style="zoom:50%;" />



##### Step 2. 实现 TCP Timestamps Option

* Timestamps 选项的大小为 10，但为了保持 TCP 的包长度为 4 的倍数，需要添加 2 个 `NOP` 的空位

  第 1 个字节为选项种类，第 2 个字节为选项长度，第 3-6 个字节为 `TSval` 表示发送端发出该报文时本地时间戳，第 7-10 字节为 `TSecr` 表示回放最近一次收到的对端报文的 `TSval` 值

  注意 `buffer` 和 `doff` 要相应增大以及主机和网络之间字节序的转换

````C++
// nop
buffer[40] = 0x01;
// nop
buffer[41] = 0x01;
// timestamp
buffer[42] = 0x08; // kind
buffer[43] = 0x0a; // length
int32_t t_sval = htonl((uint32_t)current_ts_msec());
memcpy(&buffer[44], &t_sval, sizeof(t_sval));
memcpy(&buffer[48], &tcp->t_secr, sizeof(tcp->t_secr));
````

* 实验结果

从 `lab-client.pcap` 可以看出，Timestamps 已经被 WireShark 顺利解析，并且各部分值符合规定

<img src="https://img.wzf2000.top/image/2022/05/15/2022-05-15-16.55.54.png" alt="截屏2022-05-15 16.55.54" style="zoom:50%;" />



##### Step 3. 实现 TCP New Reno 拥塞控制算法

* New Reno 算法是在 Reno 算法基础上的改进，因为 Reno 算法仅考虑了每次拥塞发生时只丢失一个报文的情况，但是在实际网络中一次拥塞中丢失多个报文的情况十分普遍

* 在 Reno 算法下，当一次拥塞中丢失多个报文时，TCP 会反复在拥塞避免和快速恢复之间转换，从而多次将 `cwnd` 和 `ssthresh` 减半，造成 TCP 的发送速率呈指数降低，系统吞吐量急剧下降

  当 `cwnd` 小于 3 时，由于没有足够的重复 `ACK` 可以触发快速恢复，只能等待超时重传，从而仅能通过传输超时来发现报文丢失

  超时对于 TCP 的影响很大，一方面在等待超时的过程中网络链路利用率很低，另一方面超时之后，`cwnd` 的值会被重设为 1，大大较低了TCP的传输效果

* 因此在 New Reno 中增加了 `Partial ACK`（部分应答）和 `Recovery ACK`（恢复应答）的概念，将一次拥塞丢失多个报文的情形与多次拥塞的情形区分开来，只有当所有报文都被应答后才退出快速恢复状态，进而在每一次拥塞发生后拥塞窗口仅减半一次，从而提高了 TCP 的顽健性和吞吐量

  具体地，New Reno 发送端在收到一个 `Partial ACK` 时，并不会立即退出快速恢复状态，而会持续地重送 `Partial ACK` 之后的数据包，直到接收到 `Recovery ACK` 再退出

  每收到一个 `Partial ACK` 时，重传定时器复位，这使得 New Reno 的发送端在网络有大量数据包遗失时不需等待超时就能更正此错误，减少大量数据包遗失对传输效果造成的影响

* 实现 `Partial ACK` 和 `Recovery ACK` 的区分，在 TCP 中增加 `recovery_ack`，记录当前 `Recover ACK` 大小，实现函数 `update_recovery_ack` 更新 `recovery_ack` 的值

  ````C++
  // tcp.h
  uint32_t recovery_ack;
  // tcp.cpp
  inline void TCP::update_recovery_ack() {
    if (recovery_ack < snd_nxt) {
      recovery_ack = snd_nxt;
    }
  }

* 修改函数 `pop_from_retransmission_queue`，增加在快速恢复阶段收到新 `ACK` 时 New Reno 算法下的处理

````C++
void TCP::pop_from_retransmission_queue(const uint32_t seg_ack) {
    ... ...
    // new ACK
    if (index != -1) {
        if (state != TCPState::SYN_SENT && state != TCPState::SYN_RCVD) {
            // update cwnd
            if (reno_state == RenoState::SLOW_START) {
                ... ...
            } else if (reno_state == RenoState::CONGESTION_AVOIDANCE) {
                ... ...
            } else {
                if (current_cc_algo == CongestionControlAlgorithm::NewReno) {
                    assert(seg_ack <= recovery_ack);
                    // Partial ACK
                    if (seg_ack < recovery_ack) {
                        // retransmit next missing packet
                        for (ssize_t i = 0, iEnd = retransmission_queue.size(); i < iEnd; i++) {
                            auto& seg = retransmission_queue[i];
                            TCPHeader *tcp_hdr = (TCPHeader *)&seg.buffer[20];
                            uint32_t seg_seq = ntohl(tcp_hdr->seq);
                            // reset timeout
                            seg.start_ms = current_ts_msec();
                            // retransmit packet
                            if (seg_seq == seg_ack) {
                                send_packet(seg.buffer, seg.header_len + seg.body_len);
                                break;
                            }
                        }
                    // Recovery ACK
                    } else {
                        cwnd = ssthresh;
                        set_reno_state(RenoState::CONGESTION_AVOIDANCE);
                    }
                } else {
                    cwnd = ssthresh;
                    set_reno_state(RenoState::CONGESTION_AVOIDANCE);
                }
            }
            // dupACKcount = 0
            clear_dup_ack_cnt();
        }
    }
}
````

* 实验结果

使用 `lab-client` 与 `lab-server` 通信，为了模拟一次拥塞中发生多次丢包情况，修改 `common.cpp` 指定 `lab-server` 在接收第 11 个和第 13 个数据包的时候发生丢失

使用 Reno 算法下，`lab-client.pcap` 表现如下：

<img src="https://img.wzf2000.top/image/2022/05/15/2022-05-15-17.36.54.png" alt="截屏2022-05-15 17.36.54" style="zoom:50%;" />

可以看出，`lab-server` 在接收 `Seq=41` 和 `Seq=51` 的数据包时发生了丢失，`lab-client` 在通过快速恢复重发了 `Seq=41` 的数据包后，在等待重复 `ACK` 的过程中触发了超时重传

打开 `lab-client.log` 可以更清晰地看到状态变化以及 `cwnd` 和 `ssthresh` 的变化趋势：

````bash
=== 1st RTT ===
# init
slow start: cwnd = 10 ssthresh = 40
=== 2nd RTT ===
# 1 ACK
slow start: cwnd = 15 ssthresh = 40
# 1 ACK
slow start: cwnd = 20 ssthresh = 40
=== 3rd RTT ===
# 1 ACK
slow start: cwnd = 25 ssthresh = 40
# 1 ACK
slow start: cwnd = 30 ssthresh = 40
# 1 ACK
slow start: cwnd = 35 ssthresh = 40
# 1 ACK
slow start: cwnd = 40 ssthresh = 40
TCP Reno state transitioned from SLOW_START to CONGESTION_AVOIDANCE
=== 4th RTT ===
# 1 ACK
# 1 ACK
# 1 ACK
# 1 ACK
congestion avoidance: cwnd = 35 ssthresh = 20
TCP Reno state transitioned from CONGESTION_AVOIDANCE to FAST_RECOVERY
# retransmit
# 1 ACK
fast retransmit: cwnd = 20 ssthresh = 20
TCP Reno state transitioned from FAST_RECOVERY to CONGESTION_AVOIDANCE
=== 5th RTT ===
# time out
TCP Reno state transitioned from CONGESTION_AVOIDANCE to SLOW_START
slow start: cwnd = 10 ssthresh = 2
=== 6th RTT ===
slow start: cwnd = 15 ssthresh = 2
````

使用 New Reno 算法下，`lab-client.pcap` 表现如下：

<img src="https://img.wzf2000.top/image/2022/05/15/2022-05-15-17.50.42.png" alt="截屏2022-05-15 17.50.42" style="zoom:50%;" />

可以看出，`lab-server` 在接收 `Seq=41` 和 `Seq=51` 的数据包时发生了丢失，`lab-client` 在通过快速恢复重发了 `Seq=41` 的数据包后，在收到 `Partial ACK` 后立刻重传了 `Seq=51` 的数据包

打开 `lab-client.log` 可以更清晰地看到状态变化以及 `cwnd` 和 `ssthresh` 的变化趋势：

````bash
# cwnd & ssthresh
=== 1st RTT ===
# init
slow start: cwnd = 10 ssthresh = 40
=== 2nd RTT ===
# 1 ACK
slow start: cwnd = 15 ssthresh = 40
# 1 ACK
slow start: cwnd = 20 ssthresh = 40
=== 3rd RTT ===
# 1 ACK
slow start: cwnd = 25 ssthresh = 40
# 1 ACK
slow start: cwnd = 30 ssthresh = 40
# 1 ACK
slow start: cwnd = 35 ssthresh = 40
# 1 ACK
slow start: cwnd = 40 ssthresh = 40
TCP Reno state transitioned from SLOW_START to CONGESTION_AVOIDANCE
=== 4th RTT ===
# 1 ACK
# 1 ACK
# 1 ACK
# 1 ACK
congestion avoidance: cwnd = 35 ssthresh = 20
TCP Reno state transitioned from CONGESTION_AVOIDANCE to FAST_RECOVERY
# retransmit
# 1 ACK
# 1 ACK
# 1 ACK
# 1 ACK
fast retransmit: cwnd = 20 ssthresh = 20
TCP Reno state transitioned from FAST_RECOVERY to CONGESTION_AVOIDANCE
````